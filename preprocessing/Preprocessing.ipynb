{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 7/7 [00:03<00:00,  1.84it/s]\n"
    }
   ],
   "source": [
    "# Cleaning the raw data\n",
    "files = glob('../datasets/TaskMaster/*.json')\n",
    "dump_file = '../datasets/TaskMaster.json'\n",
    "\n",
    "conversations = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    data = json.load(open(file))\n",
    "    for conversation in data:\n",
    "        temp = []\n",
    "        for convo in conversation['utterances']:\n",
    "            temp.append({'speaker': convo['speaker'], 'text': convo['text']})\n",
    "        conversations.append(temp)\n",
    "\n",
    "json.dump(conversations, open(dump_file, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of Conversations: 17304\nAverage Length: 19.75\nAverage length of each sentence: 41.14\nAverage number of words: 7.87\n"
    }
   ],
   "source": [
    "# Summarize interesting things about the dataset.\n",
    "data = json.load(open('../datasets/TaskMaster.json'))\n",
    "num_conversations = len(data)\n",
    "lengths = [len(i) for i in data]\n",
    "conv_average = sum(lengths) / num_conversations\n",
    "\n",
    "total = 0\n",
    "totalw = 0\n",
    "count = 0\n",
    "for conv in data:\n",
    "    for text in conv:\n",
    "        count += 1\n",
    "        total += len(text['text'])\n",
    "        totalw += len(text['text'].split())\n",
    "sent_average = round(total / count, 2)\n",
    "word_average = round(totalw / count, 2)\n",
    "\n",
    "print(f\"Number of Conversations: {num_conversations}\")\n",
    "print(f\"Average Length: {round(conv_average, 2)}\")\n",
    "print(f\"Average length of each sentence: {sent_average}\")\n",
    "print(f\"Average number of words: {word_average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['../Tokenizer/vocab.json', '../Tokenizer/merges.txt']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Train the Byte Level BPE Tokenizer.\n",
    "file = json.load(open('../datasets/TaskMaster.json'))\n",
    "\n",
    "t = ''\n",
    "for data in file:\n",
    "    for text in data:\n",
    "        t += text['speaker'] + ': ' + text['text'] + '\\n'\n",
    "\n",
    "open('../datasets/For Tokenizer/TaskMaster.txt', 'w').write(t)\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "tokenizer.train(['../datasets/For Tokenizer/wikitext.txt', '../datasets/For Tokenizer/TaskMaster.txt'], vocab_size=40000)\n",
    "tokenizer.save('../Tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenvbac630982511465c97f5b41a3a046661",
   "display_name": "Python 3.7.7 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}